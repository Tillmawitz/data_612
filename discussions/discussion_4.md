# Research Discussion Assignment 4

## Prompt

Mitigating the Harm of Recommender Systems

Read one or more of the articles below and consider how to counter the radicalizing effects of recommender systems or ways to prevent algorithmic discrimination.

Renee Diresta, Wired.com (2018): [Up Next: A Better Recommendation System](https://www.wired.com/story/creating-ethical-recommendation-engines/)

Zeynep Tufekci, The New York Times (2018): [YouTube, the Great Radicalizer](https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html)

Sanjay Krishnan, Jay Patel, Michael J. Franklin, Ken Goldberg (n/a): [Social Influence Bias in Recommender Systems: A Methodology for Learning, Analyzing, and Mitigating Bias in Ratings](https://goldberg.berkeley.edu/pubs/sanjay-recsys-v10.pdf)

## Response

### Research Discussion 4 - Matthew Tillmawitz

When discussing attempts to mitigate bias or address radicalism in recommenders, there is much discussion around algorithmic solutions to the problem. These solutions frequently come from the academic community, with the major corporate players keeping much of their data and algorithms a secret in the name of competitive advantage. Many of the methods of addressing radicalism and the dissemination of false information these companies use involve redirecting users to other reputable sources of information not controlled by the company, or community sourced information. Examples include the Wikipedia fact checking Youtube used as mentioned in the piece by Direstra, or community notes on X. This is an intentional tactic used by tech companies, as it is currently in there best interest to not address any problem in a way that could be construed as moderation. For anyone unfamiliar, [Section 230 of the Communications Act of 1934](en.wikipedia.org/wiki/Section_230) (which was added in 1996) shields all tech companies from any liability for the information or content shared on their platforms. The exact wording of the section is:

"No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider."

Tech companies have long argued that this provision is necessary as it would be impossible to actively moderate the volume of content in online platforms. This law provides legal protection from lawsuits to tech companies not available to other industries, and the major tech companies have lobbied very hard to preserve this clause. As more research is published on methods of addressing bias and radicalization algorithmically, it is becoming increasingly clear that it is no longer actually impossible for these platforms to address issues of radicalizing or biased content, it is simply that it is in their best interest not to. Radical content drives engagement, increasing the volume and value of ads on a given platform. There is no risk to the companies themselves from hosting and disseminating this content, and as a result there is no incentive for them to address it. It is actually in their best interest to avoid addressing it at all, as this could be seen as proof positive that moderation is in fact feasible and lead to greater legal liability. Without meaningful changes to the incentives for tech companies in the form of legislation, it is unlikely they will make any meaningful attempt to adopt algorithms which address the problems of radicalizing and biased content.