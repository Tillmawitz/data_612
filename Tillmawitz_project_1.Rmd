---
title: "Data 612 Project 1"
author: "Matthew Tillmawitz"
date: "2025-06-04"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(kableExtra)
```

The dataset for this project will be a toy dataset representing product reviews of 5 different products labeled "A" through "E" by different users. The users and products have varying numbers of missing reviews, and the review values themselves are designed to reflect the tendency of people to avoid the bottom range of values in rating scales. The scale ranges from 0-5 and allows for half steps in the ratings. We will be building a simple recommender that will generate product recommendations for users.

```{r make dataset}
# Defining a tibble of sample data
df <- tribble(
  ~Name, ~A, ~B, ~C, ~D, ~E,
  "Alice", 4.5, 5.0, NA, 4.5, 4.5,
  "Bob", 4.5, 4.5, NA, 3.0, 4.5,
  "Carol", 3.5, NA, 3.5, 2.0, 3.5,
  "David", NA, 4.0, 3.5, 2.5, 3.5,
  "Emma", 4.5, 5.0, NA, 3.0, 4.0,
  "Frank", 3.5, 3.5, 3.0, NA, 3.0,
  "Grace", 4.5, 4.0, 3.5, NA, 4.5,
  "Henry", NA, 3.5, 2.5, 2.5, 3.5,
  "Iris", 4.0, NA, 4.0, 3.5, 4.5,
  "Jack", 3.5, 4.0,  3.5,  3.0, 3.5
)

df |>
  kbl(caption = "User Product Ratings") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

When breaking the data into training and testing sets we want to treat each user-item pair as its own row. This ensures that we create a train-test split that has data for each user and item. We exclude the missing values, as they do not make sense for the test set and provide no predictive value in the training set.

```{r split}
set.seed(8675309)

# Pivoting dataframe longer into person-item pairings to allow for granular sampling
long_df <- df |>
  pivot_longer(cols = A:E,
               names_to = "Item",
               values_to = "Rating") |>
  filter(!is.na(Rating)) # No NAs, no imputation needed for project

# Breaking data into train-test split
train_index <- createDataPartition(y = 1:nrow(long_df),
                                   p = 0.8,
                                   list = FALSE)

train_pairs <- long_df[train_index,]
test_pairs <- long_df[-train_index,]
```

We can see the averages of each user's ratings as well as each item's average rating in the below tables. The averages are from the data in the training set only.

```{r averages}
# Calculating product averages
avg_product <- train_pairs |>
  group_by(Item) |>
  summarise(average = mean(Rating))

# Calculating user averages
avg_user <- train_pairs |>
  group_by(Name) |>
  summarise(average = mean(Rating))

avg_user |>
  kbl(caption = "Average Rating by User") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
avg_product |>
  kbl(caption = "Average Rating by Product") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Using raw averages as a blanket prediction we can calculate the RMSE on both the training and test sets to set a reference point for model accuracy. Any model we develop should have a smaller RMSE than this value in order to be useful. The RMSE on both the training and test sets are not particularly good, being off by between a half and full step in the rating scale. The test set has worse performance, as would be expected. Given that the dataset was intentionally constructed to have an uneven distribution of ratings across the possible scale, with no scores below a 2.0, the performance is even worse than one would initially assume.

```{r baseline average RMSE}
# Calculating RMSE of average score estimates
baseline_rmse_test <- sqrt(mean((test_pairs$Rating - mean(train_pairs$Rating))^2))
baseline_rmse_train <- sqrt(mean((train_pairs$Rating - mean(train_pairs$Rating))^2))

# Creating a dataframe of the RMSE for later comparison
rmse_df <- tribble(
  ~Prediction, ~RMSE,
  "Raw Average Training", baseline_rmse_train,
  "Raw Average Test", baseline_rmse_test
)

rmse_df |>
  kbl(caption = "RMSE of Predicted Ratings") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

To improve performance we can calculate the bias of each user and item to construct baseline predictors for the different user-item pairs. The bias is how much the average rating of each item or user differs from the average of the entire dataset, which we can use to adjust our estimates for each user-item pair.

```{r user bias}
# Calculating user bias scores
user_bias <- avg_user |>
  mutate(user_bias = average - mean(train_pairs$Rating)) |>
  select(Name, user_bias)

user_bias |>
  kbl(caption = "User Bias Values") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

```{r item bias}
# Calculating item bias scores
item_bias <- avg_product |>
  mutate(item_bias = average - mean(train_pairs$Rating)) |>
  select(Item, item_bias)

item_bias |>
  kbl(caption = "Item Bias Values") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

We can now generate baseline predictions by adding the item and user bias values to the set-wide average for each item-user pair. These baseline estimators are a much better starting point for a recommender, as they more accurately capture the variance in user and item ratings. By including the User and Item bias values to produce baseline predictors we can improve the RMSE of our predictions significantly. The RMSE of the predictions on both the training and test sets is approximately halved compared to the raw average predictions.

```{r baseline predictions}
# Predicting ratings in the training set using baseline estimators
train_predictions <- train_pairs |>
  left_join(user_bias, by = "Name") |>
  left_join(item_bias, by = "Item") |>
  mutate(predicted_rating = mean(train_pairs$Rating) + user_bias + item_bias)

# Predicting ratings in the testing set using baseline estimators
test_predictions <- test_pairs |>
  left_join(user_bias, by = "Name") |>
  left_join(item_bias, by = "Item") |>
  mutate(predicted_rating = mean(train_pairs$Rating) + user_bias + item_bias)
```

```{r baseline rmse}
# Calculating RMSE of the baseline estimator predictions
bias_rmse_train <-  sqrt(mean((train_predictions$Rating - train_predictions$predicted_rating)^2))
bias_rmse_test <- sqrt(mean((test_predictions$Rating - test_predictions$predicted_rating)^2))

rmse_df |>
  add_row(Prediction = "Training Baseline Predictor", RMSE = bias_rmse_train) |>
  add_row(Prediction = "Testing Baseline Predictor", RMSE = bias_rmse_test) |>
  kbl(caption = "RMSE of Predicted Ratings") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

While the baseline predictors see a significant improvement in accuracy as measured by RMSE, the performance on the testing set is still not stellar at almost a half step in the rating scale. While baseline predictors are an improvement on simple averages, they have several shortcomings. They are vulnerable to missingness, and as a result different samples of the data can have very different bias values. The tables below demonstrate this phenomena, comparing the bias values of the original sample to a new sample of the data. Additionally, simple bias adjustments do not capture more nuanced relationships in the data such as similarities between users or items which could improve estimates in situations where data is scarce.

```{r split two}
set.seed(25637)

# Breaking data into train-test split for a second time
train_index_repeat <- createDataPartition(y = 1:nrow(long_df),
                                   p = 0.8,
                                   list = FALSE)

train_pairs_repeat <- long_df[train_index_repeat,]
test_pairs_repeat <- long_df[-train_index_repeat,]

user_bias_repeat <- avg_user |>
  mutate(user_bias_repeat = average - mean(train_pairs_repeat$Rating)) |>
  select(Name, user_bias_repeat)

item_bias_repeat <- avg_product |>
  mutate(item_bias_repeat = average - mean(train_pairs_repeat$Rating)) |>
  select(Item, item_bias_repeat)

user_bias |>
  left_join(user_bias_repeat, join_by(Name)) |>
  mutate(`Absolute Difference` = abs(user_bias_repeat - user_bias),
         `Percent Difference` = abs(((user_bias_repeat - user_bias) / user_bias)) * 100) |>
  kbl(caption = "Comparison of Different Sampling on User Bias") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

item_bias |>
  left_join(item_bias_repeat, join_by(Item)) |>
  mutate(`Absolute Difference` = abs(item_bias_repeat - item_bias),
         `Percent Difference` = abs(((item_bias_repeat - item_bias) / item_bias)) * 100) |>
  kbl(caption = "Comparison of Different Sampling on Item Bias") |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

